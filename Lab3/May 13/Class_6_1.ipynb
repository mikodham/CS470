{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "Class 6-1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "L3YmWZav2Paj"
      ],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLYXxsgf2PaY"
      },
      "source": [
        "# CS470 Introduction to Artificial Intelligence\n",
        "## Deep Learning Practice \n",
        "#### TA. Yechan Hwang\n",
        "---\n",
        "\n",
        "### Agenda for this practice\n",
        "#### 1. Shakespeare dataset\n",
        "#### 2. GRU Model\n",
        "#### 3. Generating texts\n",
        "---\n",
        "<br/>\n",
        "<br/>\n",
        "<br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1c2GqEX2Pac"
      },
      "source": [
        "## 6-1. Text generation with an RNN \n",
        "In this practice, we will learn how to generate text using a character-based RNN. We will practice with a dataset of **Shakespeare's writing** (from Andrej Karpathy's [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)). We will train a model when given a sequence of characters from this data, that predicts the next character in the sequence. For example, when given the characters 'togethe', trained model will predict 'r' for the next character. Longer sequences of text can be generated by calling the model repeatedly.\n",
        "\n",
        "The following is sample output when the model in this practice trained for 30 epochs, and started with the character 'Q'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCW5u5232Pad"
      },
      "source": [
        "\n",
        "<pre>\n",
        "QUEENE:\n",
        "I had thought thou hadst a Roman; for the oracle,\n",
        "Thus by All bids the man against the word,\n",
        "Which are so weak of care, by old care done;\n",
        "Your children were in your holy love,\n",
        "And the precipitation through the bleeding throne.\n",
        "\n",
        "BISHOP OF ELY:\n",
        "Marry, and will, my lord, to weep in such a one were prettiest;\n",
        "Yet now I was adopted heir\n",
        "Of the world's lamentable day,\n",
        "To watch the next way with his father with his face?\n",
        "\n",
        "ESCALUS:\n",
        "The cause why then we are all resolved more sons.\n",
        "\n",
        "VOLUMNIA:\n",
        "O, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, it is no sin it should be dead,\n",
        "And love and pale as any will to that word.\n",
        "\n",
        "QUEEN ELIZABETH:\n",
        "But how long have I heard the soul for this world,\n",
        "And show his hands of life be proved to stand.\n",
        "\n",
        "PETRUCHIO:\n",
        "I say he look'd on, if I must be content\n",
        "To stay him from the fatal of our country's bliss.\n",
        "His lordship pluck'd from this sentence then for prey,\n",
        "And then let us twain, being the moon,\n",
        "were she such a case as fills m\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5ClNkKr2Pad"
      },
      "source": [
        "\n",
        "While most of the sentences are grammatically correct, they do not make sense. But the model seems to have learned some attributes.\n",
        "\n",
        "- Before the training, the model can't know the style of the training data. \n",
        "- But after training, the structure of the output resembles a playâ€”blocks of text generally begin with a speaker name, in all capital letters similar to the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYiU3_6A2Pae"
      },
      "source": [
        "#### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgHa-wS02eX1"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPPbaBfi2Pae"
      },
      "source": [
        "#### Download the Shakespeare dataset\n",
        "Run the following lines to download data for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcrc75242Paf",
        "outputId": "2b09532b-c5e9-43df-8e48-cfbdb835a174",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "print(path_to_file)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n",
            "/root/.keras/datasets/shakespeare.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZx8YJgL2Paf"
      },
      "source": [
        "#### Read the data\n",
        "First, let's take a look at the length of the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9q_tUEI2Pag",
        "outputId": "b69dc401-61d9-44d4-8dc8-5e1209caf558",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "print ('Length of text: {} characters'.format(len(text)))\n",
        "#from shakespeare.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 1115394 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8PogLYO2Pag"
      },
      "source": [
        "Here *length of text* is the number of characters in it. We have more than ten million characters.<br/>\n",
        "Also, we can check the first 250 characters in training text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYSlnE-S2Pag",
        "outputId": "382d6d94-4d0e-4865-93e2-79e7fecd925e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(text[:250])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MmnJr-s2Pag"
      },
      "source": [
        "Our dataset has the format of the screenplay.\n",
        "\n",
        "And how many unique characters are there? Let's check it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOx2HwDs2Pah",
        "outputId": "d842a507-2513-4112-daa2-6e2cc48abe4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "vocab = sorted(set(text))\n",
        "print ('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJI7eA8y2Pah",
        "outputId": "ecc8d4bd-d310-4727-e90b-adc7a568d462",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "vocab"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\n',\n",
              " ' ',\n",
              " '!',\n",
              " '$',\n",
              " '&',\n",
              " \"'\",\n",
              " ',',\n",
              " '-',\n",
              " '.',\n",
              " '3',\n",
              " ':',\n",
              " ';',\n",
              " '?',\n",
              " 'A',\n",
              " 'B',\n",
              " 'C',\n",
              " 'D',\n",
              " 'E',\n",
              " 'F',\n",
              " 'G',\n",
              " 'H',\n",
              " 'I',\n",
              " 'J',\n",
              " 'K',\n",
              " 'L',\n",
              " 'M',\n",
              " 'N',\n",
              " 'O',\n",
              " 'P',\n",
              " 'Q',\n",
              " 'R',\n",
              " 'S',\n",
              " 'T',\n",
              " 'U',\n",
              " 'V',\n",
              " 'W',\n",
              " 'X',\n",
              " 'Y',\n",
              " 'Z',\n",
              " 'a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'd',\n",
              " 'e',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 'j',\n",
              " 'k',\n",
              " 'l',\n",
              " 'm',\n",
              " 'n',\n",
              " 'o',\n",
              " 'p',\n",
              " 'q',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'v',\n",
              " 'w',\n",
              " 'x',\n",
              " 'y',\n",
              " 'z']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jyy0bJD72Pah"
      },
      "source": [
        "There are some special symbols and characters (including lowercase and uppercase letters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7e8pjpV2Pah"
      },
      "source": [
        "#### Vectorize the text\n",
        "Before training, we need to **map all the characters in the dataset to a numerical representation**. \n",
        "\n",
        "We will create two lookup tables: \n",
        "- one for mapping **characters to numbers**\n",
        "- another for **numbers to characters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWFKOGRO2Pah"
      },
      "source": [
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "# 1D integer vector for all characters in the text data\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_GvoNK92Pai",
        "outputId": "16149d45-78fb-4e3e-e783-9de22fc81d7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('{')\n",
        "for char,_ in zip(char2idx, range(65)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('\\n}')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  '\\n':   0,\n",
            "  ' ' :   1,\n",
            "  '!' :   2,\n",
            "  '$' :   3,\n",
            "  '&' :   4,\n",
            "  \"'\" :   5,\n",
            "  ',' :   6,\n",
            "  '-' :   7,\n",
            "  '.' :   8,\n",
            "  '3' :   9,\n",
            "  ':' :  10,\n",
            "  ';' :  11,\n",
            "  '?' :  12,\n",
            "  'A' :  13,\n",
            "  'B' :  14,\n",
            "  'C' :  15,\n",
            "  'D' :  16,\n",
            "  'E' :  17,\n",
            "  'F' :  18,\n",
            "  'G' :  19,\n",
            "  'H' :  20,\n",
            "  'I' :  21,\n",
            "  'J' :  22,\n",
            "  'K' :  23,\n",
            "  'L' :  24,\n",
            "  'M' :  25,\n",
            "  'N' :  26,\n",
            "  'O' :  27,\n",
            "  'P' :  28,\n",
            "  'Q' :  29,\n",
            "  'R' :  30,\n",
            "  'S' :  31,\n",
            "  'T' :  32,\n",
            "  'U' :  33,\n",
            "  'V' :  34,\n",
            "  'W' :  35,\n",
            "  'X' :  36,\n",
            "  'Y' :  37,\n",
            "  'Z' :  38,\n",
            "  'a' :  39,\n",
            "  'b' :  40,\n",
            "  'c' :  41,\n",
            "  'd' :  42,\n",
            "  'e' :  43,\n",
            "  'f' :  44,\n",
            "  'g' :  45,\n",
            "  'h' :  46,\n",
            "  'i' :  47,\n",
            "  'j' :  48,\n",
            "  'k' :  49,\n",
            "  'l' :  50,\n",
            "  'm' :  51,\n",
            "  'n' :  52,\n",
            "  'o' :  53,\n",
            "  'p' :  54,\n",
            "  'q' :  55,\n",
            "  'r' :  56,\n",
            "  's' :  57,\n",
            "  't' :  58,\n",
            "  'u' :  59,\n",
            "  'v' :  60,\n",
            "  'w' :  61,\n",
            "  'x' :  62,\n",
            "  'y' :  63,\n",
            "  'z' :  64,\n",
            "\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsSPs-dT2Pai",
        "outputId": "8efb9220-ac7f-4dc3-c72b-acf34b03ae0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(len(text_as_int))\n",
        "print(text_as_int)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1115394\n",
            "[18 47 56 ... 45  8  0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBVnvqEV2Pai"
      },
      "source": [
        "<br/><br/>\n",
        "Also let's check how the first 13 characters from the dataset text are mapped to integers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9g2wo5aT2Pai",
        "outputId": "b8a8151a-6924-4adb-a325-f41c55405af2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'First Citizen' ---- characters mapped to int ---- > [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3YmWZav2Paj"
      },
      "source": [
        "#### The prediction task\n",
        "Our goal is when given a character or a sequence of characters, to predict **the most probable following character.** Therefore, the **input to the model will be a sequence of characters** and the model will learn to **predict the output : the following character at each time step**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HSiQVSh2Paj"
      },
      "source": [
        "#### Create training examples and targets\n",
        "For now, divide the text into training sequences. Each input sequence will contain `seq_length` characters from the text. For each input sequence, the corresponding targets contain the same length of text, but shifted one character to the right.\n",
        "\n",
        "So break the text into chunks of `seq_length+1`. For example, let's say that `seq_length` is 4 and our training text is \"HELLO\".\n",
        "\n",
        "In this example, **the input sequence would be \"HELL\", and the target sequence \"ELLO\"**.\n",
        "\n",
        "<img src=\"https://github.com/mikodham/CS470/blob/main/Lab3/May%2013/images/teacher_forcing.png?raw=1\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
        "\n",
        "To do this, first use the [`tf.data.Dataset.from_tensor_slices`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset#from_tensor_slices) function to convert the text vector into a stream of character indices.\n",
        "- `tf.data.Dataset.from_tensor_slices`: Creates a Dataset whose elements are slices of the given tensors.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZsp1i4q2Paj",
        "outputId": "764b5f8c-5e94-4a95-c15c-2d2d413e83ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//seq_length\n",
        "\n",
        "# Make char dataset (in the form of integer)\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(5):\n",
        "    print(str(i.numpy())+\" : \"+str(idx2char[i.numpy()]))\n",
        "    "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18 : F\n",
            "47 : i\n",
            "56 : r\n",
            "57 : s\n",
            "58 : t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LCzrEvu2Paj",
        "outputId": "60b5acea-323e-4a3a-8fa6-28b70f84247f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Make sequences with sequence length +1\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "print(str(len(sequences))+\" sequences of length \"+str(seq_length+1))\n",
        "print()\n",
        "\n",
        "for item in sequences.take(5):\n",
        "    print(repr(''.join(idx2char[item.numpy()])))\n",
        "    print(len(idx2char[item.numpy()]))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11043 sequences of length 101\n",
            "\n",
            "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "101\n",
            "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "101\n",
            "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "101\n",
            "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "101\n",
            "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n",
            "101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7fGkaJZ2Paj"
      },
      "source": [
        "<br/><br/>\n",
        "\n",
        "Now we have to convert above text into input data and target data. Note that target data must be shifted one character to the right.\n",
        "\n",
        "To do this, we will use `tf.data.Dataset.map`. When we give some function to `tf.data.Dataset.map` as a parameter, it will apply the function to all elements and then return them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-0oA1tB2Pak",
        "outputId": "6f709d20-4bc2-46e4-8e03-82e6949458fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def plus_1(x):\n",
        "    return x+1\n",
        "    \n",
        "temp_dataset = tf.data.Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\n",
        "print(list(temp_dataset.as_numpy_iterator()))\n",
        "temp_dataset = temp_dataset.map(plus_1)\n",
        "print(list(temp_dataset.as_numpy_iterator()))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 2, 3, 4, 5]\n",
            "[2, 3, 4, 5, 6]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1qm_8qH2Pak",
        "outputId": "d62110c8-a3e0-440f-84c7-ba907ad11e2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "print(len(dataset))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11043\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGYGnQ302Pak",
        "outputId": "e00dd2f2-6d9c-44d5-90be-b3547da48552",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "    print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target data: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FzzcApP2Pal"
      },
      "source": [
        "<br/>\n",
        "\n",
        "During the training, **each index of these vectors are processed as one time step**. For the input at time step 0, the model receives the index for \"F\" and tries to predict the index for \"i\" as the next character. At the next timestep, it does the same thing but the **RNN considers the previous step context in addition to the current input character**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtkJdFG32Pal",
        "outputId": "a2fbb71c-7e8e-4acd-8ea0-9b3ddecbfc08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "    print(\"Step {:4d}\".format(i))\n",
        "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
        "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step    0\n",
            "  input: 18 ('F')\n",
            "  expected output: 47 ('i')\n",
            "Step    1\n",
            "  input: 47 ('i')\n",
            "  expected output: 56 ('r')\n",
            "Step    2\n",
            "  input: 56 ('r')\n",
            "  expected output: 57 ('s')\n",
            "Step    3\n",
            "  input: 57 ('s')\n",
            "  expected output: 58 ('t')\n",
            "Step    4\n",
            "  input: 58 ('t')\n",
            "  expected output: 1 (' ')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLd6JPmy2Pal"
      },
      "source": [
        "#### Create training batches\n",
        "We used `tf.data` to split the text into manageable sequences. But before feeding this data into the model, we need to **shuffle the data and pack it into batches**.\n",
        "\n",
        "[`tf.data.Dataset.shuffle`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset#shuffle)(buffer_size, seed=None, reshuffle_each_iteration=None) : Randomly shuffles the elements of this dataset.\n",
        "\n",
        "[`tf.data.Dataset.batch`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset#batch)(batch_size, drop_remainder=False) : Combines consecutive elements of this dataset into batches.\n",
        "\n",
        "Note that `tf.data.Dataset.shuffle` **doesn't shuffle characters in each sentence**, but the sentences in dataset will be shuffled by sentences.\n",
        "\n",
        "<img src=\"https://github.com/mikodham/CS470/blob/main/Lab3/May%2013/images/shuffle1.png?raw=1\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
        "\n",
        "<br/>\n",
        "<br/>\n",
        "\n",
        "<img src=\"https://github.com/mikodham/CS470/blob/main/Lab3/May%2013/images/shuffle2.png?raw=1\" alt=\"Drawing\" style=\"width: 600px;\"/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiYbNj5u2Pam",
        "outputId": "21721d99-6f70-4fdb-ccda-38157b08f3f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "# Shuffle the data and create batches (1 data = (100, 100) ==> 0:99, 1:100)\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset\n",
        "#each batch has 64 sentences, each sentence has 100 characters"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "semKsjbO2Pam"
      },
      "source": [
        "<br/><br/>\n",
        "\n",
        "We can see that each batch has 64 input sentences (each has 100 characters) and 64 target sentences (each has 100 characters). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFwP7xPD2Pam",
        "outputId": "ae6105cb-55d6-4622-a18b-be07dbf54bbb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    print(input_example_batch, target_example_batch)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[46 43 56 ... 45  1 47]\n",
            " [41 53 59 ... 32 46 43]\n",
            " [ 1 40 43 ... 57  1 58]\n",
            " ...\n",
            " [44 53 56 ... 59 56  0]\n",
            " [46 43 43 ... 43  1 61]\n",
            " [53 59  1 ... 53 51 54]], shape=(64, 100), dtype=int64) tf.Tensor(\n",
            "[[43 56  1 ...  1 47 52]\n",
            " [53 59 50 ... 46 43  1]\n",
            " [40 43 47 ...  1 58 46]\n",
            " ...\n",
            " [53 56 58 ... 56  0 42]\n",
            " [43 43  1 ...  1 61 43]\n",
            " [59  1 42 ... 51 54 43]], shape=(64, 100), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnKbM3fJ2Pam"
      },
      "source": [
        "<br/><br/>\n",
        "\n",
        "Also we can see that all the target sentences are shifted one character to the right ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYlKvK762Pam",
        "outputId": "5ffd8f96-165e-426f-fbb0-77a03905f395",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    print(input_example_batch[0], target_example_batch[0])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[ 0 21  6  1 57 47 56  2  1 52 43  5 43 56  1 39  1 61 46 47 58  8  0  0\n",
            " 24 33 15 17 26 32 21 27 10  0 13 52 42  1 52 53 58  1 39  1 48 53 58  1\n",
            " 53 44  1 32 56 39 52 47 53  1 47 52  1 63 53 59 56  1 51 53 59 58 46 10\n",
            "  0 32 56 39 52 47 53  1 47 57  1 41 46 39 52 45 43 42  1 47 52 58 53  1\n",
            " 24 59 41 43], shape=(100,), dtype=int64) tf.Tensor(\n",
            "[21  6  1 57 47 56  2  1 52 43  5 43 56  1 39  1 61 46 47 58  8  0  0 24\n",
            " 33 15 17 26 32 21 27 10  0 13 52 42  1 52 53 58  1 39  1 48 53 58  1 53\n",
            " 44  1 32 56 39 52 47 53  1 47 52  1 63 53 59 56  1 51 53 59 58 46 10  0\n",
            " 32 56 39 52 47 53  1 47 57  1 41 46 39 52 45 43 42  1 47 52 58 53  1 24\n",
            " 59 41 43 52], shape=(100,), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbdM2bx52Pan"
      },
      "source": [
        "#### Build The GRU Model\n",
        "We will use `tf.keras.Sequential` to define the model. For this simple example three layers are used to define our model:\n",
        "\n",
        "- `tf.keras.layers.Embedding`: The input layer. A trainable lookup table that will map the numbers of each character to a vector with embedding_dim dimensions;\n",
        "- [`tf.keras.layers.GRU`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/GRU): A type of RNN with size units=rnn_units (You can also use a LSTM layer here.)\n",
        "<img src=https://miro.medium.com/max/2400/1*dhq14CzJijlqjf7IlDB0uw.png>\n",
        "\n",
        "<img src=https://lh3.googleusercontent.com/proxy/sij4sv35kGVFJvWw-lwYr71iFEhXzGSxodnLNyxwKd7dpZvggXDkZoVojzIiHFgvc73qYMHAZhw0gKW-Bp-b7IVnh25hTbfpOo0lNShS9q0-3x3ujUTO8uJ1adJ1bg2p>\n",
        "\n",
        " \n",
        "- `tf.keras.layers.Dense`: The output layer, with vocab_size outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ko57Onaj2Pan"
      },
      "source": [
        "##### About the GRU\n",
        "\n",
        "GRU is a variation of LSTM. GRU has some different attributes compared to vanilla LSTM.\n",
        "\n",
        "- The two state vectors $c_t$ and $h_t$ in the LSTM Cell are merged into one vector $h_t$.\n",
        "- There is only one gate controller $z_t$ that controls all input gates.\n",
        "- There is no output gate and the state vector $h_t$ is the output of GRU.\n",
        "\n",
        "You can see details about the GRU at the this [link](https://arxiv.org/abs/1406.1078).\n",
        "In this practice, we will use GRU since its operation is faster than LSTM and it has fewer parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUybTfo-2Pan"
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab) #65\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5tyYVPq2Pan"
      },
      "source": [
        "from tensorflow.keras import Sequential \n",
        "from tensorflow.keras.layers import Embedding, GRU, Dense \n",
        "\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    ###################\n",
        "    #                 #\n",
        "    # write your code #\n",
        "    #                 # \n",
        "    model=Sequential([\n",
        "      Embedding(\n",
        "          vocab_size,embedding_dim,\n",
        "          batch_input_shape = [batch_size,None]\n",
        "      ),\n",
        "      GRU(rnn_units, \n",
        "          return_sequences=True,\n",
        "          stateful=True,\n",
        "          recurrent_initializer='glorot_uniform'\n",
        "      ),\n",
        "      Dense(vocab_size)\n",
        "    ])\n",
        "    return model\n",
        "    ###################"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHHdd3ql2Pao"
      },
      "source": [
        "model = build_model(\n",
        "    vocab_size=len(vocab), \n",
        "    embedding_dim = embedding_dim, \n",
        "    rnn_units = rnn_units, \n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "    ###################\n",
        "    #                 #\n",
        "    # write your code #\n",
        "    #                 # \n",
        "    ###################"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcR3QA5G2Pao"
      },
      "source": [
        "For each character the model looks up the embedding, runs the GRU one timestep with the embedding as input, and applies the dense layer to generate logits predicting the log-likelihood of the next character:\n",
        "\n",
        "<img src=https://www.tensorflow.org/text/tutorials/images/text_generation_training.png>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdYpjOFx2Pao"
      },
      "source": [
        "#### Try the untrained model\n",
        "Now we will run the untrained model to see how it behaves.\n",
        "\n",
        "First let's check the shape of the output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LB-4nvzB2Pao",
        "outputId": "ce9d070d-9488-4e20-a515-7684cc374f66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\") "
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mHOXOhK2Pap"
      },
      "source": [
        "<br/><br/>\n",
        "\n",
        "Also we can check the model's prediction by probability distribution. For example, we can see the model's prediction for the first sentence's fifth character.\n",
        "\n",
        "(Note that since the current model is not trained yet.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x958KMGu2Pap",
        "outputId": "b9870e90-febb-481a-ed48-7cf1bb41c464",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "example_batch_predictions[0][5]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(65,), dtype=float32, numpy=\n",
              "array([-1.27089042e-02,  1.44095663e-02,  2.00382918e-02,  1.11477487e-02,\n",
              "       -1.64984353e-03, -9.28282458e-03, -1.01572452e-02,  7.01599894e-03,\n",
              "        1.92178041e-02,  9.12852213e-03,  3.12883081e-03,  2.84779491e-03,\n",
              "        5.52502507e-03,  7.72807281e-04, -1.69514120e-03, -5.99782215e-03,\n",
              "        3.59804952e-03, -1.77651481e-03,  7.42385443e-03,  6.32357830e-03,\n",
              "       -7.25982385e-03, -1.05561735e-02,  1.38615975e-02, -6.61300682e-03,\n",
              "        7.52078649e-03, -1.37021672e-03, -2.67638499e-03, -1.21365883e-03,\n",
              "       -9.33160726e-03, -1.13348076e-02, -5.06472494e-03,  2.01825751e-03,\n",
              "        3.12439655e-03,  5.47962729e-03,  5.94117399e-03,  1.66756730e-03,\n",
              "        1.42683797e-02, -1.50501141e-02,  1.22760679e-03, -7.77127594e-03,\n",
              "       -1.46131814e-02, -1.56429992e-03, -7.85714202e-03,  4.62916167e-03,\n",
              "        8.03106185e-03, -4.87098796e-03,  2.68872059e-03, -1.27286045e-02,\n",
              "       -7.39303092e-03,  2.31492799e-04,  1.17489723e-02, -2.18379404e-03,\n",
              "        2.46502273e-03,  1.77624566e-03,  5.42647298e-03,  3.66646168e-03,\n",
              "        5.41084539e-03,  1.32397488e-02, -1.91435795e-02, -1.04982816e-02,\n",
              "        2.30179983e-03, -2.54789554e-02,  4.70722932e-03, -9.16165300e-05,\n",
              "        4.33813734e-03], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AryQ1-hb2Pap"
      },
      "source": [
        "And in our model, the sequence length of the input is 100 but the model can be run on inputs of any length.\n",
        "\n",
        "This is an advantage of the recurrent neural network which can handle inputs of variable length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMgZha-y2Pap",
        "outputId": "f4daab4f-e386-45a9-dc64-22f569c913cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           16640     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (64, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 65)            66625     \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQBvYNPm2Pap"
      },
      "source": [
        "To get actual predictions from the model, we need to sample from the output distribution to get actual character indices. This distribution is defined by the logits over the character vocabulary.\n",
        "\n",
        "Try it for the first example in the batch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoIHY1cm2Paq",
        "outputId": "b2338419-cda6-4439-edd4-5f446757b330",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# num_samples : determines how many characters to sample at each iteration\n",
        "\n",
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "print(sampled_indices)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[35]\n",
            " [22]\n",
            " [ 4]\n",
            " [20]\n",
            " [38]\n",
            " [ 0]\n",
            " [56]\n",
            " [ 7]\n",
            " [49]\n",
            " [55]\n",
            " [ 2]\n",
            " [25]\n",
            " [57]\n",
            " [56]\n",
            " [54]\n",
            " [22]\n",
            " [19]\n",
            " [52]\n",
            " [25]\n",
            " [55]\n",
            " [35]\n",
            " [34]\n",
            " [ 4]\n",
            " [38]\n",
            " [34]\n",
            " [47]\n",
            " [50]\n",
            " [ 9]\n",
            " [27]\n",
            " [28]\n",
            " [ 6]\n",
            " [42]\n",
            " [ 6]\n",
            " [19]\n",
            " [ 0]\n",
            " [ 6]\n",
            " [31]\n",
            " [34]\n",
            " [ 8]\n",
            " [45]\n",
            " [49]\n",
            " [56]\n",
            " [12]\n",
            " [44]\n",
            " [ 5]\n",
            " [55]\n",
            " [51]\n",
            " [ 3]\n",
            " [21]\n",
            " [16]\n",
            " [24]\n",
            " [11]\n",
            " [54]\n",
            " [41]\n",
            " [ 1]\n",
            " [25]\n",
            " [ 9]\n",
            " [15]\n",
            " [40]\n",
            " [45]\n",
            " [44]\n",
            " [ 9]\n",
            " [43]\n",
            " [63]\n",
            " [27]\n",
            " [57]\n",
            " [17]\n",
            " [ 0]\n",
            " [ 5]\n",
            " [12]\n",
            " [ 5]\n",
            " [30]\n",
            " [13]\n",
            " [38]\n",
            " [33]\n",
            " [ 2]\n",
            " [55]\n",
            " [ 5]\n",
            " [51]\n",
            " [32]\n",
            " [57]\n",
            " [34]\n",
            " [13]\n",
            " [ 7]\n",
            " [61]\n",
            " [46]\n",
            " [47]\n",
            " [40]\n",
            " [12]\n",
            " [26]\n",
            " [15]\n",
            " [41]\n",
            " [55]\n",
            " [38]\n",
            " [62]\n",
            " [12]\n",
            " [22]\n",
            " [43]\n",
            " [48]\n",
            " [38]], shape=(100, 1), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25faI93B2Paq"
      },
      "source": [
        "This gives us a prediction for the next character index at each timestep.\n",
        "\n",
        "Now to see the predicted sentence of our untrained model, we will squeeze the `sampled_indices` and convert them into characters.\n",
        "- [`tf.squeeze`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/squeeze): Removes dimensions of size 1 from the shape of a tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRmqa8RO2Paq",
        "outputId": "e1ac8715-50ec-4c36-c2c4-cc156d1a58b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
        "\n",
        "print(sampled_indices.shape)\n",
        "print(sampled_indices)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(100,)\n",
            "[35 22  4 20 38  0 56  7 49 55  2 25 57 56 54 22 19 52 25 55 35 34  4 38\n",
            " 34 47 50  9 27 28  6 42  6 19  0  6 31 34  8 45 49 56 12 44  5 55 51  3\n",
            " 21 16 24 11 54 41  1 25  9 15 40 45 44  9 43 63 27 57 17  0  5 12  5 30\n",
            " 13 38 33  2 55  5 51 32 57 34 13  7 61 46 47 40 12 26 15 41 55 38 62 12\n",
            " 22 43 48 38]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6v3bfdb2Paq"
      },
      "source": [
        "After sqeezing the `sampled_indices`, we got 1D vector that contains indicies of predicted characters.\n",
        "\n",
        "Let's decode this vector to see the text predicted by this untrained model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkgmAzWD2Paq",
        "outputId": "161fdfb3-e767-4c69-cfd2-6e00b90d1e6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: \n",
            " 'brother did love Juliet,\\nAnd you tell me that he shall die for it.\\n\\nANGELO:\\nHe shall not, Isabel, if'\n",
            "\n",
            "Next Char Predictions: \n",
            " \"WJ&HZ\\nr-kq!MsrpJGnMqWV&ZVil3OP,d,G\\n,SV.gkr?f'qm$IDL;pc M3Cbgf3eyOsE\\n'?'RAZU!q'mTsVA-whib?NCcqZx?JejZ\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prh2hfxK2Paq"
      },
      "source": [
        "<br/>\n",
        "\n",
        "Since our model is not trained yet, it seems to just predict next character randomly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHj4SGbn2Par"
      },
      "source": [
        "#### Train the model\n",
        "At this point the problem can be treated as a standard classification problem. **Given the previous RNN state and the input character at each time step, our model must predict the next character.**\n",
        "\n",
        "#### Compile the model\n",
        "We will use `tf.keras.losses.sparse_categorical_crossentropy` loss function since it works well for classification problem.\n",
        "\n",
        "Since our model returns logits, we need to set the `from_logits` flag."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTTo5_4U2Par",
        "outputId": "cc03966d-0f49-49f7-d396-f3be9e357795",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "# Get loss value from untrained model\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 65)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.1736755\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wa0b9XR62Par"
      },
      "source": [
        "model.compile(optimizer='adam', \n",
        "              loss=loss)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbKm5D1B2Par"
      },
      "source": [
        "#### Configure checkpoints\n",
        "Use a [`tf.keras.callbacks.ModelCheckpoint`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/callbacks/ModelCheckpoint) to ensure that checkpoints are saved during training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-KditeY2Par"
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w46MN4tr2Par",
        "outputId": "a512322a-0ef0-46d4-ba22-e286b4482a73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "history = model.fit(dataset, \n",
        "                    epochs=15, \n",
        "                    callbacks=[checkpoint_callback])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "172/172 [==============================] - 12s 56ms/step - loss: 2.6871\n",
            "Epoch 2/15\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.9543\n",
            "Epoch 3/15\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.6899\n",
            "Epoch 4/15\n",
            "172/172 [==============================] - 11s 59ms/step - loss: 1.5428\n",
            "Epoch 5/15\n",
            "172/172 [==============================] - 11s 60ms/step - loss: 1.4546\n",
            "Epoch 6/15\n",
            "172/172 [==============================] - 11s 58ms/step - loss: 1.3949\n",
            "Epoch 7/15\n",
            "172/172 [==============================] - 11s 58ms/step - loss: 1.3491\n",
            "Epoch 8/15\n",
            "172/172 [==============================] - 11s 58ms/step - loss: 1.3111\n",
            "Epoch 9/15\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.2750\n",
            "Epoch 10/15\n",
            "172/172 [==============================] - 11s 58ms/step - loss: 1.2418\n",
            "Epoch 11/15\n",
            "172/172 [==============================] - 11s 59ms/step - loss: 1.2101\n",
            "Epoch 12/15\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.1775\n",
            "Epoch 13/15\n",
            "172/172 [==============================] - 11s 58ms/step - loss: 1.1445\n",
            "Epoch 14/15\n",
            "172/172 [==============================] - 11s 59ms/step - loss: 1.1109\n",
            "Epoch 15/15\n",
            "172/172 [==============================] - 11s 60ms/step - loss: 1.0756\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWQpLiCc2Pas"
      },
      "source": [
        "#### Generate text\n",
        "We will restore the latest checkpoint. Then, to keep this prediction step simple, we will use 1 for batch size.\n",
        "\n",
        "(To run the model with a different `batch_size`, we need to rebuild the model with different batch size and restore the weights from the checkpoint.)\n",
        "\n",
        "By the codes below, we can check the path that contains weights for the lastest model and load it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mihYqtx2Pas",
        "outputId": "1b69e894-5cd3-4849-ac38-2c78e960da7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        }
      },
      "source": [
        "# Rebuild the model by changing batch size (=1) to predict new text\n",
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "# Load the weight of the model we trained \n",
        "# model.load_weights('./saved_ckpt/ckpt_15')\n",
        "\n",
        "# Change the batch size from 64 to 1\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/py_checkpoint_reader.py\u001b[0m in \u001b[0;36mNewCheckpointReader\u001b[0;34m(filepattern)\u001b[0m\n\u001b[1;32m     94\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mCheckpointReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m   \u001b[0;31m# TODO(b/143319754): Remove the RuntimeError casting logic once we resolve the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ./saved_ckpt/ckpt_15",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-5bc888eeb1e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load the weight of the model we trained\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./saved_ckpt/ckpt_15'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Change the batch size from 64 to 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[1;32m   2293\u001b[0m           'True when by_name is True.')\n\u001b[1;32m   2294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2295\u001b[0;31m     \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_detect_save_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2296\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msave_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tf'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2297\u001b[0m       \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trackable_saver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_detect_save_format\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m   2916\u001b[0m   \u001b[0;31m# directory. It's possible for filepath to be both a prefix and directory.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2917\u001b[0m   \u001b[0;31m# Prioritize checkpoint over SavedModel.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2918\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0m_is_readable_tf_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2919\u001b[0m     \u001b[0msave_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'tf'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2920\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0msm_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_is_readable_tf_checkpoint\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m   2937\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_is_readable_tf_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2938\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2939\u001b[0;31m     \u001b[0mpy_checkpoint_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNewCheckpointReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2940\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2941\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLossError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/py_checkpoint_reader.py\u001b[0m in \u001b[0;36mNewCheckpointReader\u001b[0;34m(filepattern)\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0;31m# issue with throwing python exceptions from C++.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0merror_translator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/py_checkpoint_reader.py\u001b[0m in \u001b[0;36merror_translator\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m     33\u001b[0m       \u001b[0;34m'Failed to find any '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m       'matching files for') in error_message:\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0merrors_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m   elif 'Sliced checkpoints are not supported' in error_message or (\n\u001b[1;32m     37\u001b[0m       \u001b[0;34m'Data type '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ./saved_ckpt/ckpt_15"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwHt7xhg2Pas"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VM-YSjRt2Pas"
      },
      "source": [
        "#### The prediction loop\n",
        "\n",
        "The following code block generates the text:\n",
        "\n",
        "1. Start by choosing a **start string** and initialize the RNN hidden state for the first iteration.\n",
        "2. Set the number of characters to generate.\n",
        "3. Get the **prediction distribution of the next character using the start string and hidden state**.\n",
        "4. Smaple an index of the predicted character using a multinomial distribution of the first iteration. \n",
        "5. Use this predicted character as our next input to the model.\n",
        "6. Repeat step 3-5 until we get the number of characters we set.\n",
        "\n",
        "**Note that the RNN hidden state returned by the model is fed back into the model and hidden state will become more complex as the prediction loop repeats.**\n",
        "In other words, after predicting the a word, the modified RNN states are again fed back into the model, which is how the model learns as it gets more context from the previously predicted words.\n",
        "\n",
        "\n",
        "![To generate text the model's output is fed back to the input](https://www.tensorflow.org/tutorials/text/images/text_generation_sampling.png)\n",
        "\n",
        "Looking at the generated text, you'll see the model knows when to capitalize and make paragraphs, and it imitates a Shakespeare-like writing vocabulary. With the small number of training epochs, it has not yet learned to form coherent sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ut-ffsCe2Pas"
      },
      "source": [
        "def generate_text(model, start_string, num_generate,temperature):\n",
        "    # Evaluation step (generating text using the learned model)\n",
        "\n",
        "    # Converting our start string to numbers (vectorizing)\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    # Empty string to store our results\n",
        "    text_generated = []\n",
        "\n",
        "    # Here batch size == 1\n",
        "    model.reset_states()\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        # remove the batch dimension\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "        # using a categorical distribution to predict the word returned by the model\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "        # We pass the predicted word as the next input to the model\n",
        "        # along with the previous hidden state\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "    return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njkPoVl52Pas",
        "outputId": "6ffa19cc-d1d6-4c9f-94b2-c34192b028d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "source": [
        "# Low temperatures results in more predictable text.\n",
        "# Higher temperatures results in more surprising text.\n",
        "# Experiment to find the best setting.\n",
        "\n",
        "print(generate_text(model, start_string=\"ROMEO:\", num_generate = 1000, temperature=1.0))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-a33c1a379204>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Experiment to find the best setting.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_string\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ROMEO:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_generate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-36-111c6082ca37>\u001b[0m in \u001b[0;36mgenerate_text\u001b[0;34m(model, start_string, num_generate, temperature)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Here batch size == 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'History' object has no attribute 'reset_states'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSsGH51y2Pat"
      },
      "source": [
        "<br/><br/>\n",
        "The easiest thing you can do to improve the results is to train it for longer (e.g. try EPOCHS=30).\n",
        "\n",
        "You can also experiment with a different start string, or try adding another RNN layer to improve the model's accuracy, or adjusting the temperature parameter to generate more or less random predictions."
      ]
    }
  ]
}