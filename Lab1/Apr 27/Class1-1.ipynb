{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Class1-1.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMbv7AmvLFUS"
      },
      "source": [
        "# CS470 인공지능개론\n",
        "## Deep Learning Practice \n",
        "#### Prof. Ho-Jin Choi\n",
        "#### School of Computing, KAIST\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3Pdt6ktLFUW"
      },
      "source": [
        "#  Introduction to Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3f8MYppLFUW"
      },
      "source": [
        "1. **Neural Network Basic**  \n",
        "    1-1. Linear model  \n",
        "    1-2. Multi-Layer Perceptron  \n",
        "    1-3. Deep Neural Network  \n",
        "    1-4. Training neural network\n",
        "    \n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JMkUyPzLFUX"
      },
      "source": [
        "# 1. Neural Network Basic \n",
        "\n",
        "## 1-1. Linear model \n",
        "Let's consider the binary classification.\n",
        "\n",
        "<img src=\"https://github.com/mikodham/CS470/blob/main/Lab1/Apr%2027/imgs/bt_example.png?raw=1\" align=\"center\" width=\"600\"/>\n",
        "\n",
        "To solve above problem, we can define a simple linear model as follows.\n",
        "\n",
        "<img src=\"https://github.com/mikodham/CS470/blob/main/Lab1/Apr%2027/imgs/linear_model.png?raw=1\" align=\"center\" width=\"600\"/>\n",
        "\n",
        "With the linear model, we apply it to binary classification.\n",
        "\n",
        "<img src=\"https://github.com/mikodham/CS470/blob/main/Lab1/Apr%2027/imgs/lm_classification.png?raw=1\" align=\"center\" width=\"600\"/>\n",
        "\n",
        "#### Issues of linear models\n",
        "- Most real-world data is not linearly separable\n",
        "- In other words, any linear models cannot separate regions correctly\n",
        "- Therefore, non-linearities is neceesary to model arbitrary complex functions \n",
        "\n",
        "<img src=\"https://github.com/mikodham/CS470/blob/main/Lab1/Apr%2027/imgs/issues_lm.png?raw=1\" align=\"center\" width=\"700\"/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4G-FPFEXLFUX"
      },
      "source": [
        "## 1-2. Multi-Layer Perceptron (MLP)\n",
        "\n",
        "#### Injecting non-linearity \n",
        "To solve above issues, we can inject the non-linearity into the linear model through the non-linear functions such as **Sigmoid, Hyperbolic Tangent(tanh), Rectified Linear (ReLU).**\n",
        "\n",
        "<img src=\"https://github.com/mikodham/CS470/blob/main/Lab1/Apr%2027/imgs/inject_non_liearity.png?raw=1\" align=\"center\" width=\"600\"/>\n",
        "\n",
        "<img src=\"https://github.com/mikodham/CS470/blob/main/Lab1/Apr%2027/imgs/act_functions.png?raw=1\" align=\"center\" width=\"600\"/>\n",
        "\n",
        "If you use sigmoid-like activation functions, like sigmoid and tanh, after some epochs of training, the linear part of each neuron will have values that are very big or very small. This means that the linear part will have a big output value regardless of its sign. Consequently, the input of sigmoid-like functions in each neuron which adds non-linearity will be far from the center of these functions.\n",
        "Sigmoid function 0 to 1, binary classification\n",
        "\n",
        "#### Perceptron: simplified view \n",
        "Perceptron: A Perceptron is an algorithm used for supervised learning of binary classifiers. Binary classifiers decide whether an input, usually represented by a series of vectors, belongs to a specific class. In short, a perceptron is a single-layer neural network. (Defined by Deep AI)\n",
        "\n",
        "<img src=\"https://github.com/mikodham/CS470/blob/main/Lab1/Apr%2027/imgs/perceptrons.png?raw=1\" align=\"center\" width=\"700\"/>\n",
        "\n",
        "\n",
        "#### Multi-Layer Perceptron \n",
        "MLP consists of at least three layers of nodes: an input layer, a hidden layer and an output layer. \n",
        "\n",
        "<img src=\"https://github.com/mikodham/CS470/blob/main/Lab1/Apr%2027/imgs/mlp.png?raw=1\" align=\"center\" width=\"700\"/>\n",
        "\n",
        "\n",
        "The reason why do we stack more layers is that 1) hidden layers are nonlinear embeddings of the input; 2) The model can embed the data into\n",
        "the linearly separable space.\n",
        "\n",
        "<img src=\"https://github.com/mikodham/CS470/blob/main/Lab1/Apr%2027/imgs/mlp2.png?raw=1\" align=\"center\" width=\"700\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2aXvdYULFUY"
      },
      "source": [
        "## 1-3. Deep Neural Network (DNN)\n",
        "By stacking the more and more layers, neural netowrks have representing and modeling ability for given complex data.\n",
        "\n",
        "<img src=\"https://github.com/mikodham/CS470/blob/main/Lab1/Apr%2027/imgs/dnn.png?raw=1\" align=\"center\" width=\"700\"/>\n",
        "\n",
        "\n",
        "#### Example: recognizing handwritten digits\n",
        "\n",
        "Examples of data are as follows (MNIST)\n",
        "\n",
        "<img src=\"https://github.com/mikodham/CS470/blob/main/Lab1/Apr%2027/imgs/handwritten_digits.jpeg?raw=1\" align=\"center\" width=\"600\"/>\n",
        "\n",
        "\n",
        "To recognize the handwritten digits using DNN, we first preprocess the images in order to feed them to the model and then train the model.\n",
        "\n",
        "**Data representation**\n",
        "Representing a gray-scale image into an array (i.e. a vector)\n",
        "\n",
        "<img src=\"https://github.com/mikodham/CS470/blob/main/Lab1/Apr%2027/imgs/data_representation.png?raw=1\" align=\"center\" width=\"700\"/>\n",
        "\n",
        "\n",
        "**Forward propagation (Embedding viewpoint)**\n",
        "The vectorized image is propagated through the layers and classified into one of the digits, 0~9. In the case of trained DNN, each layer represents non-linear mebedding of input to easily separable space. \n",
        "\n",
        "<img src=\"https://github.com/mikodham/CS470/blob/main/Lab1/Apr%2027/imgs/fp_mnist.png?raw=1\" align=\"center\" width=\"700\"/>\n",
        "\n",
        "\n",
        "**Recognition viewpoint**\n",
        "The DNN model understands given handwritten digit by combining the abstracted features represented through multiple layers. In other words, DNN learns a hierarchy of features capturing different levels of abstractions. \n",
        "\n",
        "<img src=\"https://github.com/mikodham/CS470/blob/main/Lab1/Apr%2027/imgs/recognition_viewpoint.png?raw=1\" align=\"center\" width=\"700\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ived2zidLFUY"
      },
      "source": [
        "## 1-4. Training neural network \n",
        "\n",
        "- **Objective:** find a set of parameters that minimize the error on the dataset.\n",
        "- Notations\n",
        "    - Datasets: ${(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}), \\dots, (x^{(N)},y^{(N)})}$ for $N$ number of training data\n",
        "    - Parameters: ${\\text{w}^{(1)},\\text{w}^{(2)}, \\dots, \\text{w}^{(L)}}$ for $L$ number of layers\n",
        "    \n",
        "   \n",
        "<img src=\"https://github.com/mikodham/CS470/blob/main/Lab1/Apr%2027/imgs/dnn_training1.png?raw=1\" align=\"center\" width=\"600\"/>\n",
        "\n",
        "\n",
        "- **Loss function**\n",
        "    - Measurement on the mismatch between the model prediction and the true label.\n",
        "    - There are many ways to define the degree of mismatch (i.e. misprediction, error).\n",
        "    - Key to “quantify the performance” of the model on the specific task and data.\n",
        "    \n",
        "<img src=\"https://github.com/mikodham/CS470/blob/main/Lab1/Apr%2027/imgs/loss_func.png?raw=1\" align=\"center\" width=\"600\"/>\n",
        "    \n",
        "    \n",
        "- Example of loss function - Mean Squared Error(MSE)\n",
        "    - Regression tasks\n",
        "\n",
        "<img src=\"https://github.com/mikodham/CS470/blob/main/Lab1/Apr%2027/imgs/mse.png?raw=1\" align=\"center\" width=\"600\"/>\n",
        "  \n",
        "  \n",
        "- Example of loss function - Binary cross entropy \n",
        "    - Binary classification \n",
        "    - For predicted values, can be interpreted as a probability vector using softmax function \n",
        "\n",
        "<img src=\"https://github.com/mikodham/CS470/blob/main/Lab1/Apr%2027/imgs/bce.png?raw=1\" align=\"center\" width=\"600\"/>\n",
        "\n",
        "\n",
        "#### Optimizing the loss function (Back-propagation)\n",
        "- **Challenges** for optimizing the loss function\n",
        "    - It is highly non-convex and non-concave.\n",
        "    - It is impossible to find the analytical solution.\n",
        "    \n",
        "- **Optimiation via gradient descent**\n",
        "\n",
        "<img src=\"https://github.com/mikodham/CS470/blob/main/Lab1/Apr%2027/imgs/gradient_descent.png?raw=1\" align=\"center\" width=\"600\"/>\n",
        "\n",
        "- Algorithm (gradient descent)\n",
        "\n",
        "<img src=\"https://github.com/mikodham/CS470/blob/main/Lab1/Apr%2027/imgs/gd_a1.png?raw=1\" align=\"center\" width=\"600\"/>\n",
        "\n",
        "- Algorithm (stochastic gradient descent)\n",
        "\n",
        "<img src=\"https://github.com/mikodham/CS470/blob/main/Lab1/Apr%2027/imgs/gd_a2.png?raw=1\" align=\"center\" width=\"600\"/>\n",
        "\n",
        "- **Algorithm (minibatch stochastic gradient descent)**\n",
        "\n",
        "<img src=\"https://github.com/mikodham/CS470/blob/main/Lab1/Apr%2027/imgs/gd_a3.png?raw=1\" align=\"center\" width=\"600\"/>\n",
        "\n",
        "\n",
        "\n",
        "#### Computing gradients of weights in neural network \n",
        "- Chain rule: propagating the gradient across the layers\n",
        "    - Simplest example: two-layer neural network with one hidden node\n",
        "    - $\\hat{y}=f(x;\\text{W})$\n",
        "    \n",
        "<img src=\"https://github.com/mikodham/CS470/blob/main/Lab1/Apr%2027/imgs/chain_rule.png?raw=1\" align=\"center\" width=\"700\"/>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KX_4qHIFLFUZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}